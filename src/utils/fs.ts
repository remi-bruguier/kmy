import fs from "fs-extra";
import path from "path";
import { URL } from "url";
import { ScrapedPage } from "../crawler/types.js";

export async function savePage(
  page: ScrapedPage,
  outputDir: string
): Promise<void> {
  const { url, title, content } = page;
  const parsedUrl = new URL(url);
  let relativePath = parsedUrl.pathname.replace(/\/$/, "");
  if (!relativePath) relativePath = "index";
  const outputPath = path.join(outputDir, `${relativePath}.md`);
  await fs.ensureDir(path.dirname(outputPath));
  const markdown = `# ${title}\n\nSource URL: ${url}\n\n## Content\n\n${content}\n`;
  await fs.writeFile(outputPath, markdown);
}

export async function createIndex(
  pages: Array<{ url: string; title: string }>,
  outputDir: string
): Promise<void> {
  const indexPath = path.join(outputDir, "README.md");
  const markdown = `# Scraped Pages\n\nTotal pages: ${
    pages.length
  }\n\n## Pages\n\n${pages
    .map((page) => `- [${page.title}](${page.url})`)
    .join("\n")}\n\n---\n*Generated by Kemoway Scraper*\n`;
  await fs.writeFile(indexPath, markdown);
}
